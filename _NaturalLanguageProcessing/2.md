---
layout: collection
author: DiscreteTom
catalog: true
title: 'N-gram语言模型'
collection: NaturalLanguageProcessing
---

## 概率语言模型

### 基础

如何评价词串`W=w1, w2, w3, ..., wn`的质量（即合法性）

**计算W出现的概率`p(W)`**，即`p(w1, w2, ..., wn)`。根据概率链式法则：

`p(W) = p(w1)p(w2|w1)...p(wn|w1, w2, ..., wn-1)`

### 极大似然估计

Maximum Likelihood Estimation, MLE

![2-1](../img/2-1.png)

如评估句子I love you，需要计算`p(you|I,love)=C(I love you)/C(I love)`，即"I love you"的出现次数除以"I love"的出现次数

### 缺陷与解决方案

当前词出现的概率依赖于前面的词。如果串中词量很大
- 可能导致某个子串出现次数为0，导致概率无法计算
- 随着串的增加，概率越来越小

一个可能的解决方案：当前词的出现概率 **仅依赖较短的历史词**

## n-gram语言模型

### 基础

马尔科夫假设：位于某个特定状态的概率取决于（约等于）前n-1个状态。应用于语言模型：**假设每个词的出现只与前面的n-1个词有关，即n-gram模型（n元语法/n元文法）**

- 1-gram语言模型(unigram) - 0阶马尔科夫链
	- 与前面的0个词有关
	- p(W)=p(w1)p(w2)...p(wn)
- 2-gram语言模型(bigrams) - 1阶马尔科夫链
- 3-gram语言模型(trigrams) - 2阶马尔科夫链

### n-gram模型的参数n

关于n-gram中n的取值。显然n越大生成的句子越好，但是n过大会导致模型不可行，原因是**计算量太大**以及**概率为0的项太多**

**实际应用常取n=3**

**参数的估计**：给定一个训练数据集X（和n），从中计算参数

## 数据稀疏及平滑

有些训练数据集X中的一些词高频出现，一些词低频出现，造成估计结果不可靠。也可能有词不出现导致概率不可计算

增加训练语料？增加的语料中高频词仍然占绝大部分

主要解决方案：
- **平滑Smoothing**：重新估计零概率及低值概率，给它们非零值。即给没有见过的事件分配未来发生的可能性
- **回退Back-off**：高阶n-grams概率难以计算时使用低阶n-grams来统计

### Zipf定律（略

Zipf's law：在自然语言的语料库里，一个单词的出现频率与它在频率表里的排名成反比。假设排名表中单词w1, w2, w4的出现频率为最多、第二多、第四多，那么
- w1的频率约为w2的2倍
- w2的频率约为w4的2倍

### Laplace Smoothing(Add-one)

以 **unigram** 为例，令`ci`为词`wi`出现的次数，`N`为训练数据集中的单词总量，`|V|`为词表大小（即训练数据集中不同的单词的数量）。原始的p(wi)=ci/N。

对词表中的每一个单词的出现次数进行+1处理后，训练数据集中的单词总量修正为`N + |V|`，则

![2-2](../img/2-2.png)

或者，换一种思路去理解，p的分母仍然是N，分子则是ci的修正值：

![2-3](../img/2-3.png)

其中ci的修正值（也称为 **加1折扣计数discount**）为

![2-4](../img/2-4.png)

同理，在 **bigram** 里面，一个原始的MLE值:

![2-5](../img/2-5.png)

针对所有的两个单词组成的单词对c(wi, wj)，其出现频率+1，得到的修正概率：

![2-6](../img/2-6.png)

其中V显然应为单词对的数量。也可以像unigram一样改变理解方式：

![2-7](../img/2-7.png)

其中c的修正值：

![2-8](../img/2-8.png)

### Lidstone smoothing(Add-Delta)

是add-one平滑的一种扩展，把add-one中的1改为delta或δ，取值范围`0 <= delta <= 1`

显然delta=1时即为Laplace平滑

如何选取合适的delta?常用 **held-out estimation(保守估计)**
1. 从训练数据D中分离出一部分数据H(held-out data/validation data)
2. 采用数据H训练具有不同delta值的语言模型
3. 分别测试这些模型在H上的表现
4. 选取最优模型的delta作为最优delta

### Good-Turing Smoothing

**以unigram为例**

根据仅出现一次的unigram的个数来确定未出现的unigram的概率。

现规定仅出现一次的unigram的个数N1:

![2-9](../img/2-9.png)

出现c次的unigram的个数Nc:

![2-10](../img/2-10.png)

规定出现c次的词出现次数的修正值（即折扣后的值）为

![2-11](../img/2-11.png)

那么其出现概率应为修正值除N，即

![2-12](../img/2-12.png)

出现0次的词的折扣后出现次数为

![2-13](../img/2-13.png)

则出现0次的词的出现概率为

![2-14](../img/2-14.png)

**对于n-gram**

类似地，令nr表示出现r次的n-grams的个数。则出现r次的单词的出现次数修正为

![2-15](../img/2-15.png)

Good-Turing的归一化特性证明：

![2-17](../img/2-17.png)

### 回退法Backoff

基本思路：长度为n的串出现次数为0时，使用**长度为n-1的以同样单词结尾的串的修正后的出现次数**

![2-18](../img/2-18.png)

其中α1和α2就是用于修正的因子

### 插值法Interpolation（略

基本思路：把**同样单词结尾的长度不大于原串长n的串**都纳入考虑范围，加权得到修正值

![2-19](../img/2-19.png)

### KN平滑（略

思想：出现次数多的词更可能出现在新的上下文中

以词w出现在新的上下文中的概率来取代w的出现概率

